[2021-06-16 14:43:04 swin_small_patch4_window7_224] (main.py 342): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2021-06-16 14:43:04 swin_small_patch4_window7_224] (main.py 345): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /workspace/mnt/storage/yankai/imagenet/ImageNet-pytorch
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2021-06-16 14:43:07 swin_small_patch4_window7_224] (main.py 79): INFO Creating model:swin/swin_small_patch4_window7_224
[2021-06-16 14:43:08 swin_small_patch4_window7_224] (main.py 82): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2021-06-16 14:43:08 swin_small_patch4_window7_224] (main.py 91): INFO number of params: 49606258
[2021-06-16 14:43:08 swin_small_patch4_window7_224] (main.py 94): INFO number of GFLOPs: 8.746520064
[2021-06-16 14:43:08 swin_small_patch4_window7_224] (main.py 118): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2021-06-16 14:43:08 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth....................
[2021-06-16 14:43:08 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-06-16 14:43:10 swin_small_patch4_window7_224] (main.py 268): INFO Test: [0/391]	Time 1.697 (1.697)	Loss 0.6338 (0.6338)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)	Mem 1596MB
[2021-06-16 14:43:11 swin_small_patch4_window7_224] (main.py 268): INFO Test: [10/391]	Time 0.137 (0.294)	Loss 0.8448 (0.7586)	Acc@1 81.250 (83.594)	Acc@5 94.531 (95.384)	Mem 1696MB
[2021-06-16 14:43:13 swin_small_patch4_window7_224] (main.py 268): INFO Test: [20/391]	Time 0.138 (0.221)	Loss 0.5611 (0.7336)	Acc@1 87.500 (83.705)	Acc@5 98.438 (95.908)	Mem 1696MB
[2021-06-16 14:43:14 swin_small_patch4_window7_224] (main.py 268): INFO Test: [30/391]	Time 0.138 (0.194)	Loss 0.8017 (0.7323)	Acc@1 84.375 (83.745)	Acc@5 94.531 (95.842)	Mem 1696MB
[2021-06-16 14:43:16 swin_small_patch4_window7_224] (main.py 268): INFO Test: [40/391]	Time 0.138 (0.181)	Loss 0.5868 (0.7513)	Acc@1 87.500 (83.213)	Acc@5 97.656 (95.903)	Mem 1696MB
[2021-06-16 14:43:17 swin_small_patch4_window7_224] (main.py 268): INFO Test: [50/391]	Time 0.138 (0.173)	Loss 0.8524 (0.7511)	Acc@1 81.250 (83.349)	Acc@5 93.750 (95.910)	Mem 1696MB
[2021-06-16 14:43:18 swin_small_patch4_window7_224] (main.py 268): INFO Test: [60/391]	Time 0.139 (0.167)	Loss 0.6387 (0.7528)	Acc@1 86.719 (83.363)	Acc@5 97.656 (95.889)	Mem 1696MB
[2021-06-16 14:43:20 swin_small_patch4_window7_224] (main.py 268): INFO Test: [70/391]	Time 0.139 (0.163)	Loss 0.7187 (0.7533)	Acc@1 82.031 (83.143)	Acc@5 95.312 (95.885)	Mem 1696MB
[2021-06-16 14:43:21 swin_small_patch4_window7_224] (main.py 268): INFO Test: [80/391]	Time 0.138 (0.160)	Loss 0.6027 (0.7440)	Acc@1 82.812 (83.266)	Acc@5 98.438 (96.017)	Mem 1696MB
[2021-06-16 14:43:23 swin_small_patch4_window7_224] (main.py 268): INFO Test: [90/391]	Time 0.141 (0.158)	Loss 0.7577 (0.7444)	Acc@1 82.812 (83.199)	Acc@5 95.312 (96.042)	Mem 1696MB
[2021-06-16 14:43:24 swin_small_patch4_window7_224] (main.py 268): INFO Test: [100/391]	Time 0.138 (0.156)	Loss 0.7772 (0.7400)	Acc@1 82.812 (83.331)	Acc@5 94.531 (96.078)	Mem 1696MB
[2021-06-16 14:43:25 swin_small_patch4_window7_224] (main.py 268): INFO Test: [110/391]	Time 0.138 (0.155)	Loss 0.6495 (0.7391)	Acc@1 85.938 (83.277)	Acc@5 97.656 (96.094)	Mem 1696MB
[2021-06-16 14:43:27 swin_small_patch4_window7_224] (main.py 268): INFO Test: [120/391]	Time 0.138 (0.153)	Loss 0.6951 (0.7398)	Acc@1 84.375 (83.187)	Acc@5 99.219 (96.126)	Mem 1696MB
[2021-06-16 14:43:28 swin_small_patch4_window7_224] (main.py 268): INFO Test: [130/391]	Time 0.139 (0.152)	Loss 0.6170 (0.7366)	Acc@1 85.156 (83.224)	Acc@5 98.438 (96.189)	Mem 1696MB
[2021-06-16 14:43:30 swin_small_patch4_window7_224] (main.py 268): INFO Test: [140/391]	Time 0.142 (0.151)	Loss 0.5930 (0.7395)	Acc@1 85.156 (83.211)	Acc@5 96.875 (96.160)	Mem 1696MB
[2021-06-16 14:43:31 swin_small_patch4_window7_224] (main.py 268): INFO Test: [150/391]	Time 0.139 (0.151)	Loss 0.6390 (0.7390)	Acc@1 85.938 (83.257)	Acc@5 96.094 (96.120)	Mem 1696MB
[2021-06-16 14:43:33 swin_small_patch4_window7_224] (main.py 268): INFO Test: [160/391]	Time 0.139 (0.152)	Loss 0.7642 (0.7395)	Acc@1 85.156 (83.269)	Acc@5 93.750 (96.108)	Mem 1696MB
[2021-06-16 14:43:34 swin_small_patch4_window7_224] (main.py 268): INFO Test: [170/391]	Time 0.139 (0.151)	Loss 0.7483 (0.7353)	Acc@1 84.375 (83.356)	Acc@5 94.531 (96.117)	Mem 1696MB
[2021-06-16 14:43:35 swin_small_patch4_window7_224] (main.py 268): INFO Test: [180/391]	Time 0.141 (0.150)	Loss 0.7400 (0.7379)	Acc@1 80.469 (83.292)	Acc@5 96.094 (96.107)	Mem 1696MB
[2021-06-16 14:43:37 swin_small_patch4_window7_224] (main.py 268): INFO Test: [190/391]	Time 0.139 (0.150)	Loss 0.7306 (0.7357)	Acc@1 77.344 (83.279)	Acc@5 97.656 (96.122)	Mem 1696MB
[2021-06-16 14:43:38 swin_small_patch4_window7_224] (main.py 268): INFO Test: [200/391]	Time 0.147 (0.149)	Loss 0.6897 (0.7322)	Acc@1 82.031 (83.329)	Acc@5 96.094 (96.137)	Mem 1696MB
[2021-06-16 14:43:40 swin_small_patch4_window7_224] (main.py 268): INFO Test: [210/391]	Time 0.139 (0.149)	Loss 0.7511 (0.7322)	Acc@1 83.594 (83.357)	Acc@5 95.312 (96.127)	Mem 1696MB
[2021-06-16 14:43:41 swin_small_patch4_window7_224] (main.py 268): INFO Test: [220/391]	Time 0.142 (0.149)	Loss 0.5003 (0.7313)	Acc@1 86.719 (83.368)	Acc@5 100.000 (96.154)	Mem 1696MB
[2021-06-16 14:43:42 swin_small_patch4_window7_224] (main.py 268): INFO Test: [230/391]	Time 0.140 (0.148)	Loss 0.8885 (0.7337)	Acc@1 78.906 (83.262)	Acc@5 93.750 (96.128)	Mem 1696MB
[2021-06-16 14:43:44 swin_small_patch4_window7_224] (main.py 268): INFO Test: [240/391]	Time 0.139 (0.148)	Loss 0.8530 (0.7391)	Acc@1 82.812 (83.198)	Acc@5 93.750 (96.087)	Mem 1696MB
[2021-06-16 14:43:45 swin_small_patch4_window7_224] (main.py 268): INFO Test: [250/391]	Time 0.139 (0.148)	Loss 0.6780 (0.7379)	Acc@1 85.938 (83.195)	Acc@5 96.094 (96.106)	Mem 1696MB
[2021-06-16 14:43:47 swin_small_patch4_window7_224] (main.py 268): INFO Test: [260/391]	Time 0.140 (0.147)	Loss 0.6471 (0.7363)	Acc@1 83.594 (83.214)	Acc@5 97.656 (96.142)	Mem 1696MB
[2021-06-16 14:43:48 swin_small_patch4_window7_224] (main.py 268): INFO Test: [270/391]	Time 0.140 (0.147)	Loss 0.5228 (0.7338)	Acc@1 88.281 (83.251)	Acc@5 96.875 (96.163)	Mem 1696MB
[2021-06-16 14:43:49 swin_small_patch4_window7_224] (main.py 268): INFO Test: [280/391]	Time 0.141 (0.147)	Loss 0.8027 (0.7350)	Acc@1 83.594 (83.246)	Acc@5 93.750 (96.138)	Mem 1696MB
[2021-06-16 14:43:51 swin_small_patch4_window7_224] (main.py 268): INFO Test: [290/391]	Time 0.139 (0.147)	Loss 0.6097 (0.7355)	Acc@1 87.500 (83.226)	Acc@5 97.656 (96.145)	Mem 1696MB
[2021-06-16 14:43:52 swin_small_patch4_window7_224] (main.py 268): INFO Test: [300/391]	Time 0.140 (0.146)	Loss 0.7353 (0.7346)	Acc@1 82.031 (83.241)	Acc@5 96.875 (96.156)	Mem 1696MB
[2021-06-16 14:43:54 swin_small_patch4_window7_224] (main.py 268): INFO Test: [310/391]	Time 0.143 (0.146)	Loss 0.5475 (0.7339)	Acc@1 86.719 (83.245)	Acc@5 96.875 (96.154)	Mem 1696MB
[2021-06-16 14:43:55 swin_small_patch4_window7_224] (main.py 268): INFO Test: [320/391]	Time 0.140 (0.146)	Loss 0.7023 (0.7344)	Acc@1 84.375 (83.241)	Acc@5 96.875 (96.147)	Mem 1696MB
[2021-06-16 14:43:56 swin_small_patch4_window7_224] (main.py 268): INFO Test: [330/391]	Time 0.139 (0.146)	Loss 0.6464 (0.7326)	Acc@1 81.250 (83.261)	Acc@5 97.656 (96.181)	Mem 1696MB
[2021-06-16 14:43:58 swin_small_patch4_window7_224] (main.py 268): INFO Test: [340/391]	Time 0.140 (0.146)	Loss 0.6625 (0.7325)	Acc@1 84.375 (83.268)	Acc@5 96.875 (96.188)	Mem 1696MB
[2021-06-16 14:43:59 swin_small_patch4_window7_224] (main.py 268): INFO Test: [350/391]	Time 0.140 (0.146)	Loss 0.5204 (0.7320)	Acc@1 88.281 (83.262)	Acc@5 97.656 (96.194)	Mem 1696MB
[2021-06-16 14:44:01 swin_small_patch4_window7_224] (main.py 268): INFO Test: [360/391]	Time 0.148 (0.145)	Loss 0.8791 (0.7327)	Acc@1 80.469 (83.224)	Acc@5 96.875 (96.198)	Mem 1696MB
[2021-06-16 14:44:02 swin_small_patch4_window7_224] (main.py 268): INFO Test: [370/391]	Time 0.140 (0.145)	Loss 0.6847 (0.7328)	Acc@1 87.500 (83.225)	Acc@5 98.438 (96.231)	Mem 1696MB
[2021-06-16 14:44:04 swin_small_patch4_window7_224] (main.py 268): INFO Test: [380/391]	Time 0.143 (0.145)	Loss 0.7560 (0.7343)	Acc@1 85.156 (83.208)	Acc@5 95.312 (96.231)	Mem 1696MB
[2021-06-16 14:44:05 swin_small_patch4_window7_224] (main.py 268): INFO Test: [390/391]	Time 0.109 (0.145)	Loss 0.8189 (0.7345)	Acc@1 80.000 (83.174)	Acc@5 95.000 (96.230)	Mem 1696MB
[2021-06-16 14:44:05 swin_small_patch4_window7_224] (main.py 274): INFO  * Acc@1 83.174 Acc@5 96.230
[2021-06-16 14:44:05 swin_small_patch4_window7_224] (main.py 123): INFO Accuracy of the network on the 50000 test images: 83.2%
[2021-06-16 15:11:37 swin_small_patch4_window7_224] (main.py 342): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2021-06-16 15:11:37 swin_small_patch4_window7_224] (main.py 345): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /workspace/mnt/storage/yankai/imagenet/ImageNet-pytorch
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2021-06-16 15:11:40 swin_small_patch4_window7_224] (main.py 79): INFO Creating model:swin/swin_small_patch4_window7_224
[2021-06-16 15:11:41 swin_small_patch4_window7_224] (main.py 82): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2021-06-16 15:11:41 swin_small_patch4_window7_224] (main.py 91): INFO number of params: 49606258
[2021-06-16 15:11:41 swin_small_patch4_window7_224] (main.py 94): INFO number of GFLOPs: 8.746520064
[2021-06-16 15:11:41 swin_small_patch4_window7_224] (main.py 118): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2021-06-16 15:11:41 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth....................
[2021-06-16 15:11:42 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-06-16 15:11:43 swin_small_patch4_window7_224] (main.py 268): INFO Test: [0/391]	Time 1.388 (1.388)	Loss 0.6338 (0.6338)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)	Mem 1596MB
[2021-06-16 15:11:45 swin_small_patch4_window7_224] (main.py 268): INFO Test: [10/391]	Time 0.136 (0.273)	Loss 0.8448 (0.7586)	Acc@1 81.250 (83.594)	Acc@5 94.531 (95.384)	Mem 1696MB
[2021-06-16 15:11:46 swin_small_patch4_window7_224] (main.py 268): INFO Test: [20/391]	Time 0.137 (0.209)	Loss 0.5611 (0.7336)	Acc@1 87.500 (83.705)	Acc@5 98.438 (95.908)	Mem 1696MB
[2021-06-16 15:11:47 swin_small_patch4_window7_224] (main.py 268): INFO Test: [30/391]	Time 0.137 (0.186)	Loss 0.8017 (0.7323)	Acc@1 84.375 (83.745)	Acc@5 94.531 (95.842)	Mem 1696MB
[2021-06-16 15:11:49 swin_small_patch4_window7_224] (main.py 268): INFO Test: [40/391]	Time 0.147 (0.174)	Loss 0.5868 (0.7513)	Acc@1 87.500 (83.213)	Acc@5 97.656 (95.903)	Mem 1696MB
[2021-06-16 15:11:50 swin_small_patch4_window7_224] (main.py 268): INFO Test: [50/391]	Time 0.138 (0.167)	Loss 0.8524 (0.7511)	Acc@1 81.250 (83.349)	Acc@5 93.750 (95.910)	Mem 1696MB
[2021-06-16 15:11:51 swin_small_patch4_window7_224] (main.py 268): INFO Test: [60/391]	Time 0.138 (0.163)	Loss 0.6387 (0.7528)	Acc@1 86.719 (83.363)	Acc@5 97.656 (95.889)	Mem 1696MB
[2021-06-16 15:11:53 swin_small_patch4_window7_224] (main.py 268): INFO Test: [70/391]	Time 0.139 (0.159)	Loss 0.7187 (0.7533)	Acc@1 82.031 (83.143)	Acc@5 95.312 (95.885)	Mem 1696MB
[2021-06-16 15:11:54 swin_small_patch4_window7_224] (main.py 268): INFO Test: [80/391]	Time 0.144 (0.157)	Loss 0.6027 (0.7440)	Acc@1 82.812 (83.266)	Acc@5 98.438 (96.017)	Mem 1696MB
[2021-06-16 15:11:56 swin_small_patch4_window7_224] (main.py 268): INFO Test: [90/391]	Time 0.144 (0.155)	Loss 0.7577 (0.7444)	Acc@1 82.812 (83.199)	Acc@5 95.312 (96.042)	Mem 1696MB
[2021-06-16 15:11:57 swin_small_patch4_window7_224] (main.py 268): INFO Test: [100/391]	Time 0.138 (0.153)	Loss 0.7772 (0.7400)	Acc@1 82.812 (83.331)	Acc@5 94.531 (96.078)	Mem 1696MB
[2021-06-16 15:11:58 swin_small_patch4_window7_224] (main.py 268): INFO Test: [110/391]	Time 0.138 (0.152)	Loss 0.6495 (0.7391)	Acc@1 85.938 (83.277)	Acc@5 97.656 (96.094)	Mem 1696MB
[2021-06-16 15:12:00 swin_small_patch4_window7_224] (main.py 268): INFO Test: [120/391]	Time 0.140 (0.151)	Loss 0.6951 (0.7398)	Acc@1 84.375 (83.187)	Acc@5 99.219 (96.126)	Mem 1696MB
[2021-06-16 15:12:01 swin_small_patch4_window7_224] (main.py 268): INFO Test: [130/391]	Time 0.140 (0.150)	Loss 0.6170 (0.7366)	Acc@1 85.156 (83.224)	Acc@5 98.438 (96.189)	Mem 1696MB
[2021-06-16 15:12:03 swin_small_patch4_window7_224] (main.py 268): INFO Test: [140/391]	Time 0.138 (0.149)	Loss 0.5930 (0.7395)	Acc@1 85.156 (83.211)	Acc@5 96.875 (96.160)	Mem 1696MB
[2021-06-16 15:12:04 swin_small_patch4_window7_224] (main.py 268): INFO Test: [150/391]	Time 0.138 (0.148)	Loss 0.6390 (0.7390)	Acc@1 85.938 (83.257)	Acc@5 96.094 (96.120)	Mem 1696MB
[2021-06-16 15:12:05 swin_small_patch4_window7_224] (main.py 268): INFO Test: [160/391]	Time 0.144 (0.148)	Loss 0.7642 (0.7395)	Acc@1 85.156 (83.269)	Acc@5 93.750 (96.108)	Mem 1696MB
[2021-06-16 15:12:07 swin_small_patch4_window7_224] (main.py 268): INFO Test: [170/391]	Time 0.139 (0.148)	Loss 0.7483 (0.7353)	Acc@1 84.375 (83.356)	Acc@5 94.531 (96.117)	Mem 1696MB
[2021-06-16 15:12:08 swin_small_patch4_window7_224] (main.py 268): INFO Test: [180/391]	Time 0.139 (0.147)	Loss 0.7400 (0.7379)	Acc@1 80.469 (83.292)	Acc@5 96.094 (96.107)	Mem 1696MB
[2021-06-16 15:12:10 swin_small_patch4_window7_224] (main.py 268): INFO Test: [190/391]	Time 0.138 (0.147)	Loss 0.7306 (0.7357)	Acc@1 77.344 (83.279)	Acc@5 97.656 (96.122)	Mem 1696MB
[2021-06-16 15:12:11 swin_small_patch4_window7_224] (main.py 268): INFO Test: [200/391]	Time 0.138 (0.147)	Loss 0.6897 (0.7322)	Acc@1 82.031 (83.329)	Acc@5 96.094 (96.137)	Mem 1696MB
[2021-06-16 15:12:12 swin_small_patch4_window7_224] (main.py 268): INFO Test: [210/391]	Time 0.138 (0.146)	Loss 0.7511 (0.7322)	Acc@1 83.594 (83.357)	Acc@5 95.312 (96.127)	Mem 1696MB
[2021-06-16 15:12:14 swin_small_patch4_window7_224] (main.py 268): INFO Test: [220/391]	Time 0.139 (0.146)	Loss 0.5003 (0.7313)	Acc@1 86.719 (83.368)	Acc@5 100.000 (96.154)	Mem 1696MB
[2021-06-16 15:12:15 swin_small_patch4_window7_224] (main.py 268): INFO Test: [230/391]	Time 0.146 (0.146)	Loss 0.8885 (0.7337)	Acc@1 78.906 (83.262)	Acc@5 93.750 (96.128)	Mem 1696MB
[2021-06-16 15:12:17 swin_small_patch4_window7_224] (main.py 268): INFO Test: [240/391]	Time 0.139 (0.145)	Loss 0.8530 (0.7391)	Acc@1 82.812 (83.198)	Acc@5 93.750 (96.087)	Mem 1696MB
[2021-06-16 15:12:18 swin_small_patch4_window7_224] (main.py 268): INFO Test: [250/391]	Time 0.139 (0.145)	Loss 0.6780 (0.7379)	Acc@1 85.938 (83.195)	Acc@5 96.094 (96.106)	Mem 1696MB
[2021-06-16 15:12:19 swin_small_patch4_window7_224] (main.py 268): INFO Test: [260/391]	Time 0.138 (0.145)	Loss 0.6471 (0.7363)	Acc@1 83.594 (83.214)	Acc@5 97.656 (96.142)	Mem 1696MB
[2021-06-16 15:12:21 swin_small_patch4_window7_224] (main.py 268): INFO Test: [270/391]	Time 0.141 (0.145)	Loss 0.5228 (0.7338)	Acc@1 88.281 (83.251)	Acc@5 96.875 (96.163)	Mem 1696MB
[2021-06-16 15:12:22 swin_small_patch4_window7_224] (main.py 268): INFO Test: [280/391]	Time 0.139 (0.145)	Loss 0.8027 (0.7350)	Acc@1 83.594 (83.246)	Acc@5 93.750 (96.138)	Mem 1696MB
[2021-06-16 15:12:24 swin_small_patch4_window7_224] (main.py 268): INFO Test: [290/391]	Time 0.149 (0.145)	Loss 0.6097 (0.7355)	Acc@1 87.500 (83.226)	Acc@5 97.656 (96.145)	Mem 1696MB
[2021-06-16 15:12:25 swin_small_patch4_window7_224] (main.py 268): INFO Test: [300/391]	Time 0.139 (0.144)	Loss 0.7353 (0.7346)	Acc@1 82.031 (83.241)	Acc@5 96.875 (96.156)	Mem 1696MB
[2021-06-16 15:12:26 swin_small_patch4_window7_224] (main.py 268): INFO Test: [310/391]	Time 0.139 (0.144)	Loss 0.5475 (0.7339)	Acc@1 86.719 (83.245)	Acc@5 96.875 (96.154)	Mem 1696MB
[2021-06-16 15:12:28 swin_small_patch4_window7_224] (main.py 268): INFO Test: [320/391]	Time 0.141 (0.144)	Loss 0.7023 (0.7344)	Acc@1 84.375 (83.241)	Acc@5 96.875 (96.147)	Mem 1696MB
[2021-06-16 15:12:29 swin_small_patch4_window7_224] (main.py 268): INFO Test: [330/391]	Time 0.139 (0.144)	Loss 0.6464 (0.7326)	Acc@1 81.250 (83.261)	Acc@5 97.656 (96.181)	Mem 1696MB
[2021-06-16 15:12:31 swin_small_patch4_window7_224] (main.py 268): INFO Test: [340/391]	Time 0.140 (0.144)	Loss 0.6625 (0.7325)	Acc@1 84.375 (83.268)	Acc@5 96.875 (96.188)	Mem 1696MB
[2021-06-16 15:12:32 swin_small_patch4_window7_224] (main.py 268): INFO Test: [350/391]	Time 0.139 (0.144)	Loss 0.5204 (0.7320)	Acc@1 88.281 (83.262)	Acc@5 97.656 (96.194)	Mem 1696MB
[2021-06-16 15:12:33 swin_small_patch4_window7_224] (main.py 268): INFO Test: [360/391]	Time 0.139 (0.144)	Loss 0.8791 (0.7327)	Acc@1 80.469 (83.224)	Acc@5 96.875 (96.198)	Mem 1696MB
[2021-06-16 15:12:35 swin_small_patch4_window7_224] (main.py 268): INFO Test: [370/391]	Time 0.143 (0.144)	Loss 0.6847 (0.7328)	Acc@1 87.500 (83.225)	Acc@5 98.438 (96.231)	Mem 1696MB
[2021-06-16 15:12:36 swin_small_patch4_window7_224] (main.py 268): INFO Test: [380/391]	Time 0.139 (0.144)	Loss 0.7560 (0.7343)	Acc@1 85.156 (83.208)	Acc@5 95.312 (96.231)	Mem 1696MB
[2021-06-16 15:12:38 swin_small_patch4_window7_224] (main.py 268): INFO Test: [390/391]	Time 0.123 (0.143)	Loss 0.8189 (0.7345)	Acc@1 80.000 (83.174)	Acc@5 95.000 (96.230)	Mem 1696MB
[2021-06-16 15:12:38 swin_small_patch4_window7_224] (main.py 274): INFO  * Acc@1 83.174 Acc@5 96.230
[2021-06-16 15:12:38 swin_small_patch4_window7_224] (main.py 123): INFO Accuracy of the network on the 50000 test images: 83.2%
[2021-06-16 15:12:59 swin_small_patch4_window7_224] (main.py 342): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2021-06-16 15:12:59 swin_small_patch4_window7_224] (main.py 345): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /workspace/mnt/storage/yankai/imagenet/ImageNet-pytorch
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2021-06-16 15:13:03 swin_small_patch4_window7_224] (main.py 79): INFO Creating model:swin/swin_small_patch4_window7_224
[2021-06-16 15:13:03 swin_small_patch4_window7_224] (main.py 82): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2021-06-16 15:13:03 swin_small_patch4_window7_224] (main.py 91): INFO number of params: 49606258
[2021-06-16 15:13:03 swin_small_patch4_window7_224] (main.py 94): INFO number of GFLOPs: 8.746520064
[2021-06-16 15:13:03 swin_small_patch4_window7_224] (main.py 118): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2021-06-16 15:13:03 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth....................
[2021-06-16 15:13:04 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-06-16 15:13:05 swin_small_patch4_window7_224] (main.py 268): INFO Test: [0/391]	Time 1.399 (1.399)	Loss 0.6338 (0.6338)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)	Mem 1596MB
[2021-06-16 15:13:07 swin_small_patch4_window7_224] (main.py 268): INFO Test: [10/391]	Time 0.138 (0.268)	Loss 0.8448 (0.7586)	Acc@1 81.250 (83.594)	Acc@5 94.531 (95.384)	Mem 1696MB
[2021-06-16 15:13:08 swin_small_patch4_window7_224] (main.py 268): INFO Test: [20/391]	Time 0.138 (0.209)	Loss 0.5611 (0.7336)	Acc@1 87.500 (83.705)	Acc@5 98.438 (95.908)	Mem 1696MB
[2021-06-16 15:13:09 swin_small_patch4_window7_224] (main.py 268): INFO Test: [30/391]	Time 0.139 (0.187)	Loss 0.8017 (0.7323)	Acc@1 84.375 (83.745)	Acc@5 94.531 (95.842)	Mem 1696MB
[2021-06-16 15:13:11 swin_small_patch4_window7_224] (main.py 268): INFO Test: [40/391]	Time 0.140 (0.175)	Loss 0.5868 (0.7513)	Acc@1 87.500 (83.213)	Acc@5 97.656 (95.903)	Mem 1696MB
[2021-06-16 15:13:12 swin_small_patch4_window7_224] (main.py 268): INFO Test: [50/391]	Time 0.139 (0.168)	Loss 0.8524 (0.7511)	Acc@1 81.250 (83.349)	Acc@5 93.750 (95.910)	Mem 1696MB
[2021-06-16 15:13:14 swin_small_patch4_window7_224] (main.py 268): INFO Test: [60/391]	Time 0.139 (0.164)	Loss 0.6387 (0.7528)	Acc@1 86.719 (83.363)	Acc@5 97.656 (95.889)	Mem 1696MB
[2021-06-16 15:13:15 swin_small_patch4_window7_224] (main.py 268): INFO Test: [70/391]	Time 0.139 (0.160)	Loss 0.7187 (0.7533)	Acc@1 82.031 (83.143)	Acc@5 95.312 (95.885)	Mem 1696MB
[2021-06-16 15:13:16 swin_small_patch4_window7_224] (main.py 268): INFO Test: [80/391]	Time 0.139 (0.158)	Loss 0.6027 (0.7440)	Acc@1 82.812 (83.266)	Acc@5 98.438 (96.017)	Mem 1696MB
[2021-06-16 15:13:18 swin_small_patch4_window7_224] (main.py 268): INFO Test: [90/391]	Time 0.139 (0.156)	Loss 0.7577 (0.7444)	Acc@1 82.812 (83.199)	Acc@5 95.312 (96.042)	Mem 1696MB
[2021-06-16 15:13:19 swin_small_patch4_window7_224] (main.py 268): INFO Test: [100/391]	Time 0.139 (0.154)	Loss 0.7772 (0.7400)	Acc@1 82.812 (83.331)	Acc@5 94.531 (96.078)	Mem 1696MB
[2021-06-16 15:13:21 swin_small_patch4_window7_224] (main.py 268): INFO Test: [110/391]	Time 0.139 (0.153)	Loss 0.6495 (0.7391)	Acc@1 85.938 (83.277)	Acc@5 97.656 (96.094)	Mem 1696MB
[2021-06-16 15:13:22 swin_small_patch4_window7_224] (main.py 268): INFO Test: [120/391]	Time 0.142 (0.152)	Loss 0.6951 (0.7398)	Acc@1 84.375 (83.187)	Acc@5 99.219 (96.126)	Mem 1696MB
[2021-06-16 15:13:23 swin_small_patch4_window7_224] (main.py 268): INFO Test: [130/391]	Time 0.139 (0.151)	Loss 0.6170 (0.7366)	Acc@1 85.156 (83.224)	Acc@5 98.438 (96.189)	Mem 1696MB
[2021-06-16 15:13:25 swin_small_patch4_window7_224] (main.py 268): INFO Test: [140/391]	Time 0.140 (0.150)	Loss 0.5930 (0.7395)	Acc@1 85.156 (83.211)	Acc@5 96.875 (96.160)	Mem 1696MB
[2021-06-16 15:13:26 swin_small_patch4_window7_224] (main.py 268): INFO Test: [150/391]	Time 0.140 (0.150)	Loss 0.6390 (0.7390)	Acc@1 85.938 (83.257)	Acc@5 96.094 (96.120)	Mem 1696MB
[2021-06-16 15:13:28 swin_small_patch4_window7_224] (main.py 268): INFO Test: [160/391]	Time 0.142 (0.150)	Loss 0.7642 (0.7395)	Acc@1 85.156 (83.269)	Acc@5 93.750 (96.108)	Mem 1696MB
[2021-06-16 15:13:29 swin_small_patch4_window7_224] (main.py 268): INFO Test: [170/391]	Time 0.140 (0.149)	Loss 0.7483 (0.7353)	Acc@1 84.375 (83.356)	Acc@5 94.531 (96.117)	Mem 1696MB
[2021-06-16 15:13:31 swin_small_patch4_window7_224] (main.py 268): INFO Test: [180/391]	Time 0.140 (0.149)	Loss 0.7400 (0.7379)	Acc@1 80.469 (83.292)	Acc@5 96.094 (96.107)	Mem 1696MB
[2021-06-16 15:13:32 swin_small_patch4_window7_224] (main.py 268): INFO Test: [190/391]	Time 0.139 (0.148)	Loss 0.7306 (0.7357)	Acc@1 77.344 (83.279)	Acc@5 97.656 (96.122)	Mem 1696MB
[2021-06-16 15:13:33 swin_small_patch4_window7_224] (main.py 268): INFO Test: [200/391]	Time 0.146 (0.148)	Loss 0.6897 (0.7322)	Acc@1 82.031 (83.329)	Acc@5 96.094 (96.137)	Mem 1696MB
[2021-06-16 15:13:35 swin_small_patch4_window7_224] (main.py 268): INFO Test: [210/391]	Time 0.140 (0.148)	Loss 0.7511 (0.7322)	Acc@1 83.594 (83.357)	Acc@5 95.312 (96.127)	Mem 1696MB
[2021-06-16 15:13:36 swin_small_patch4_window7_224] (main.py 268): INFO Test: [220/391]	Time 0.140 (0.147)	Loss 0.5003 (0.7313)	Acc@1 86.719 (83.368)	Acc@5 100.000 (96.154)	Mem 1696MB
[2021-06-16 15:13:38 swin_small_patch4_window7_224] (main.py 268): INFO Test: [230/391]	Time 0.139 (0.147)	Loss 0.8885 (0.7337)	Acc@1 78.906 (83.262)	Acc@5 93.750 (96.128)	Mem 1696MB
[2021-06-16 15:13:39 swin_small_patch4_window7_224] (main.py 268): INFO Test: [240/391]	Time 0.140 (0.147)	Loss 0.8530 (0.7391)	Acc@1 82.812 (83.198)	Acc@5 93.750 (96.087)	Mem 1696MB
[2021-06-16 15:13:40 swin_small_patch4_window7_224] (main.py 268): INFO Test: [250/391]	Time 0.141 (0.147)	Loss 0.6780 (0.7379)	Acc@1 85.938 (83.195)	Acc@5 96.094 (96.106)	Mem 1696MB
[2021-06-16 15:13:42 swin_small_patch4_window7_224] (main.py 268): INFO Test: [260/391]	Time 0.144 (0.146)	Loss 0.6471 (0.7363)	Acc@1 83.594 (83.214)	Acc@5 97.656 (96.142)	Mem 1696MB
[2021-06-16 15:13:43 swin_small_patch4_window7_224] (main.py 268): INFO Test: [270/391]	Time 0.141 (0.146)	Loss 0.5228 (0.7338)	Acc@1 88.281 (83.251)	Acc@5 96.875 (96.163)	Mem 1696MB
[2021-06-16 15:13:45 swin_small_patch4_window7_224] (main.py 268): INFO Test: [280/391]	Time 0.144 (0.146)	Loss 0.8027 (0.7350)	Acc@1 83.594 (83.246)	Acc@5 93.750 (96.138)	Mem 1696MB
[2021-06-16 15:13:46 swin_small_patch4_window7_224] (main.py 268): INFO Test: [290/391]	Time 0.140 (0.146)	Loss 0.6097 (0.7355)	Acc@1 87.500 (83.226)	Acc@5 97.656 (96.145)	Mem 1696MB
[2021-06-16 15:13:47 swin_small_patch4_window7_224] (main.py 268): INFO Test: [300/391]	Time 0.140 (0.146)	Loss 0.7353 (0.7346)	Acc@1 82.031 (83.241)	Acc@5 96.875 (96.156)	Mem 1696MB
[2021-06-16 15:13:49 swin_small_patch4_window7_224] (main.py 268): INFO Test: [310/391]	Time 0.140 (0.145)	Loss 0.5475 (0.7339)	Acc@1 86.719 (83.245)	Acc@5 96.875 (96.154)	Mem 1696MB
[2021-06-16 15:13:50 swin_small_patch4_window7_224] (main.py 268): INFO Test: [320/391]	Time 0.147 (0.145)	Loss 0.7023 (0.7344)	Acc@1 84.375 (83.241)	Acc@5 96.875 (96.147)	Mem 1696MB
[2021-06-16 15:13:52 swin_small_patch4_window7_224] (main.py 268): INFO Test: [330/391]	Time 0.140 (0.145)	Loss 0.6464 (0.7326)	Acc@1 81.250 (83.261)	Acc@5 97.656 (96.181)	Mem 1696MB
[2021-06-16 15:13:53 swin_small_patch4_window7_224] (main.py 268): INFO Test: [340/391]	Time 0.140 (0.145)	Loss 0.6625 (0.7325)	Acc@1 84.375 (83.268)	Acc@5 96.875 (96.188)	Mem 1696MB
[2021-06-16 15:13:55 swin_small_patch4_window7_224] (main.py 268): INFO Test: [350/391]	Time 0.140 (0.145)	Loss 0.5204 (0.7320)	Acc@1 88.281 (83.262)	Acc@5 97.656 (96.194)	Mem 1696MB
[2021-06-16 15:13:56 swin_small_patch4_window7_224] (main.py 268): INFO Test: [360/391]	Time 0.140 (0.145)	Loss 0.8791 (0.7327)	Acc@1 80.469 (83.224)	Acc@5 96.875 (96.198)	Mem 1696MB
[2021-06-16 15:13:57 swin_small_patch4_window7_224] (main.py 268): INFO Test: [370/391]	Time 0.140 (0.145)	Loss 0.6847 (0.7328)	Acc@1 87.500 (83.225)	Acc@5 98.438 (96.231)	Mem 1696MB
[2021-06-16 15:13:59 swin_small_patch4_window7_224] (main.py 268): INFO Test: [380/391]	Time 0.140 (0.145)	Loss 0.7560 (0.7343)	Acc@1 85.156 (83.208)	Acc@5 95.312 (96.231)	Mem 1696MB
[2021-06-16 15:14:00 swin_small_patch4_window7_224] (main.py 268): INFO Test: [390/391]	Time 0.106 (0.144)	Loss 0.8189 (0.7345)	Acc@1 80.000 (83.174)	Acc@5 95.000 (96.230)	Mem 1696MB
[2021-06-16 15:14:00 swin_small_patch4_window7_224] (main.py 274): INFO  * Acc@1 83.174 Acc@5 96.230
[2021-06-16 15:14:00 swin_small_patch4_window7_224] (main.py 123): INFO Accuracy of the network on the 50000 test images: 83.2%
[2021-06-16 18:13:43 swin_small_patch4_window7_224] (main.py 342): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2021-06-16 18:13:43 swin_small_patch4_window7_224] (main.py 345): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /workspace/mnt/storage/yankai/imagenet/ImageNet-pytorch
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2021-06-16 18:13:47 swin_small_patch4_window7_224] (main.py 79): INFO Creating model:swin/swin_small_patch4_window7_224
[2021-06-16 18:13:48 swin_small_patch4_window7_224] (main.py 82): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2021-06-16 18:13:48 swin_small_patch4_window7_224] (main.py 91): INFO number of params: 49606258
[2021-06-16 18:13:48 swin_small_patch4_window7_224] (main.py 94): INFO number of GFLOPs: 8.746520064
[2021-06-16 18:13:48 swin_small_patch4_window7_224] (main.py 118): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2021-06-16 18:13:48 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth....................
[2021-06-16 18:13:48 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-06-16 18:13:50 swin_small_patch4_window7_224] (main.py 268): INFO Test: [0/391]	Time 1.599 (1.599)	Loss 0.6338 (0.6338)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)	Mem 1596MB
[2021-06-16 18:13:51 swin_small_patch4_window7_224] (main.py 268): INFO Test: [10/391]	Time 0.137 (0.287)	Loss 0.8448 (0.7586)	Acc@1 81.250 (83.594)	Acc@5 94.531 (95.384)	Mem 1696MB
[2021-06-16 18:13:53 swin_small_patch4_window7_224] (main.py 268): INFO Test: [20/391]	Time 0.138 (0.218)	Loss 0.5611 (0.7336)	Acc@1 87.500 (83.705)	Acc@5 98.438 (95.908)	Mem 1696MB
[2021-06-16 18:13:54 swin_small_patch4_window7_224] (main.py 268): INFO Test: [30/391]	Time 0.137 (0.192)	Loss 0.8017 (0.7323)	Acc@1 84.375 (83.745)	Acc@5 94.531 (95.842)	Mem 1696MB
[2021-06-16 18:13:55 swin_small_patch4_window7_224] (main.py 268): INFO Test: [40/391]	Time 0.137 (0.179)	Loss 0.5868 (0.7513)	Acc@1 87.500 (83.213)	Acc@5 97.656 (95.903)	Mem 1696MB
[2021-06-16 18:13:57 swin_small_patch4_window7_224] (main.py 268): INFO Test: [50/391]	Time 0.137 (0.171)	Loss 0.8524 (0.7511)	Acc@1 81.250 (83.349)	Acc@5 93.750 (95.910)	Mem 1696MB
[2021-06-16 18:13:58 swin_small_patch4_window7_224] (main.py 268): INFO Test: [60/391]	Time 0.138 (0.166)	Loss 0.6387 (0.7528)	Acc@1 86.719 (83.363)	Acc@5 97.656 (95.889)	Mem 1696MB
[2021-06-16 18:13:59 swin_small_patch4_window7_224] (main.py 268): INFO Test: [70/391]	Time 0.137 (0.162)	Loss 0.7187 (0.7533)	Acc@1 82.031 (83.143)	Acc@5 95.312 (95.885)	Mem 1696MB
[2021-06-16 18:14:01 swin_small_patch4_window7_224] (main.py 268): INFO Test: [80/391]	Time 0.138 (0.159)	Loss 0.6027 (0.7440)	Acc@1 82.812 (83.266)	Acc@5 98.438 (96.017)	Mem 1696MB
[2021-06-16 18:14:02 swin_small_patch4_window7_224] (main.py 268): INFO Test: [90/391]	Time 0.137 (0.157)	Loss 0.7577 (0.7444)	Acc@1 82.812 (83.199)	Acc@5 95.312 (96.042)	Mem 1696MB
[2021-06-16 18:14:04 swin_small_patch4_window7_224] (main.py 268): INFO Test: [100/391]	Time 0.137 (0.155)	Loss 0.7772 (0.7400)	Acc@1 82.812 (83.331)	Acc@5 94.531 (96.078)	Mem 1696MB
[2021-06-16 18:14:05 swin_small_patch4_window7_224] (main.py 268): INFO Test: [110/391]	Time 0.140 (0.153)	Loss 0.6495 (0.7391)	Acc@1 85.938 (83.277)	Acc@5 97.656 (96.094)	Mem 1696MB
[2021-06-16 18:14:06 swin_small_patch4_window7_224] (main.py 268): INFO Test: [120/391]	Time 0.140 (0.152)	Loss 0.6951 (0.7398)	Acc@1 84.375 (83.187)	Acc@5 99.219 (96.126)	Mem 1696MB
[2021-06-16 18:14:08 swin_small_patch4_window7_224] (main.py 268): INFO Test: [130/391]	Time 0.138 (0.151)	Loss 0.6170 (0.7366)	Acc@1 85.156 (83.224)	Acc@5 98.438 (96.189)	Mem 1696MB
[2021-06-16 18:14:09 swin_small_patch4_window7_224] (main.py 268): INFO Test: [140/391]	Time 0.138 (0.150)	Loss 0.5930 (0.7395)	Acc@1 85.156 (83.211)	Acc@5 96.875 (96.160)	Mem 1696MB
[2021-06-16 18:14:11 swin_small_patch4_window7_224] (main.py 268): INFO Test: [150/391]	Time 0.138 (0.150)	Loss 0.6390 (0.7390)	Acc@1 85.938 (83.257)	Acc@5 96.094 (96.120)	Mem 1696MB
[2021-06-16 18:14:12 swin_small_patch4_window7_224] (main.py 268): INFO Test: [160/391]	Time 0.147 (0.150)	Loss 0.7642 (0.7395)	Acc@1 85.156 (83.269)	Acc@5 93.750 (96.108)	Mem 1696MB
[2021-06-16 18:14:13 swin_small_patch4_window7_224] (main.py 268): INFO Test: [170/391]	Time 0.140 (0.149)	Loss 0.7483 (0.7353)	Acc@1 84.375 (83.356)	Acc@5 94.531 (96.117)	Mem 1696MB
[2021-06-16 18:14:15 swin_small_patch4_window7_224] (main.py 268): INFO Test: [180/391]	Time 0.139 (0.149)	Loss 0.7400 (0.7379)	Acc@1 80.469 (83.292)	Acc@5 96.094 (96.107)	Mem 1696MB
[2021-06-16 18:14:16 swin_small_patch4_window7_224] (main.py 268): INFO Test: [190/391]	Time 0.139 (0.148)	Loss 0.7306 (0.7357)	Acc@1 77.344 (83.279)	Acc@5 97.656 (96.122)	Mem 1696MB
[2021-06-16 18:14:18 swin_small_patch4_window7_224] (main.py 268): INFO Test: [200/391]	Time 0.148 (0.148)	Loss 0.6897 (0.7322)	Acc@1 82.031 (83.329)	Acc@5 96.094 (96.137)	Mem 1696MB
[2021-06-16 18:14:19 swin_small_patch4_window7_224] (main.py 268): INFO Test: [210/391]	Time 0.139 (0.147)	Loss 0.7511 (0.7322)	Acc@1 83.594 (83.357)	Acc@5 95.312 (96.127)	Mem 1696MB
[2021-06-16 18:14:20 swin_small_patch4_window7_224] (main.py 268): INFO Test: [220/391]	Time 0.138 (0.147)	Loss 0.5003 (0.7313)	Acc@1 86.719 (83.368)	Acc@5 100.000 (96.154)	Mem 1696MB
[2021-06-16 18:14:22 swin_small_patch4_window7_224] (main.py 268): INFO Test: [230/391]	Time 0.138 (0.147)	Loss 0.8885 (0.7337)	Acc@1 78.906 (83.262)	Acc@5 93.750 (96.128)	Mem 1696MB
[2021-06-16 18:14:23 swin_small_patch4_window7_224] (main.py 268): INFO Test: [240/391]	Time 0.138 (0.146)	Loss 0.8530 (0.7391)	Acc@1 82.812 (83.198)	Acc@5 93.750 (96.087)	Mem 1696MB
[2021-06-16 18:14:25 swin_small_patch4_window7_224] (main.py 268): INFO Test: [250/391]	Time 0.160 (0.146)	Loss 0.6780 (0.7379)	Acc@1 85.938 (83.195)	Acc@5 96.094 (96.106)	Mem 1696MB
[2021-06-16 18:14:26 swin_small_patch4_window7_224] (main.py 268): INFO Test: [260/391]	Time 0.141 (0.146)	Loss 0.6471 (0.7363)	Acc@1 83.594 (83.214)	Acc@5 97.656 (96.142)	Mem 1696MB
[2021-06-16 18:14:28 swin_small_patch4_window7_224] (main.py 268): INFO Test: [270/391]	Time 0.139 (0.146)	Loss 0.5228 (0.7338)	Acc@1 88.281 (83.251)	Acc@5 96.875 (96.163)	Mem 1696MB
[2021-06-16 18:14:29 swin_small_patch4_window7_224] (main.py 268): INFO Test: [280/391]	Time 0.140 (0.146)	Loss 0.8027 (0.7350)	Acc@1 83.594 (83.246)	Acc@5 93.750 (96.138)	Mem 1696MB
[2021-06-16 18:14:30 swin_small_patch4_window7_224] (main.py 268): INFO Test: [290/391]	Time 0.143 (0.145)	Loss 0.6097 (0.7355)	Acc@1 87.500 (83.226)	Acc@5 97.656 (96.145)	Mem 1696MB
[2021-06-16 18:14:32 swin_small_patch4_window7_224] (main.py 268): INFO Test: [300/391]	Time 0.140 (0.145)	Loss 0.7353 (0.7346)	Acc@1 82.031 (83.241)	Acc@5 96.875 (96.156)	Mem 1696MB
[2021-06-16 18:14:33 swin_small_patch4_window7_224] (main.py 268): INFO Test: [310/391]	Time 0.141 (0.145)	Loss 0.5475 (0.7339)	Acc@1 86.719 (83.245)	Acc@5 96.875 (96.154)	Mem 1696MB
[2021-06-16 18:14:35 swin_small_patch4_window7_224] (main.py 268): INFO Test: [320/391]	Time 0.139 (0.145)	Loss 0.7023 (0.7344)	Acc@1 84.375 (83.241)	Acc@5 96.875 (96.147)	Mem 1696MB
[2021-06-16 18:14:36 swin_small_patch4_window7_224] (main.py 268): INFO Test: [330/391]	Time 0.139 (0.145)	Loss 0.6464 (0.7326)	Acc@1 81.250 (83.261)	Acc@5 97.656 (96.181)	Mem 1696MB
[2021-06-16 18:14:37 swin_small_patch4_window7_224] (main.py 268): INFO Test: [340/391]	Time 0.141 (0.145)	Loss 0.6625 (0.7325)	Acc@1 84.375 (83.268)	Acc@5 96.875 (96.188)	Mem 1696MB
[2021-06-16 18:14:39 swin_small_patch4_window7_224] (main.py 268): INFO Test: [350/391]	Time 0.139 (0.144)	Loss 0.5204 (0.7320)	Acc@1 88.281 (83.262)	Acc@5 97.656 (96.194)	Mem 1696MB
[2021-06-16 18:14:40 swin_small_patch4_window7_224] (main.py 268): INFO Test: [360/391]	Time 0.143 (0.144)	Loss 0.8791 (0.7327)	Acc@1 80.469 (83.224)	Acc@5 96.875 (96.198)	Mem 1696MB
[2021-06-16 18:14:42 swin_small_patch4_window7_224] (main.py 268): INFO Test: [370/391]	Time 0.140 (0.144)	Loss 0.6847 (0.7328)	Acc@1 87.500 (83.225)	Acc@5 98.438 (96.231)	Mem 1696MB
[2021-06-16 18:14:43 swin_small_patch4_window7_224] (main.py 268): INFO Test: [380/391]	Time 0.139 (0.144)	Loss 0.7560 (0.7343)	Acc@1 85.156 (83.208)	Acc@5 95.312 (96.231)	Mem 1696MB
[2021-06-16 18:14:44 swin_small_patch4_window7_224] (main.py 268): INFO Test: [390/391]	Time 0.118 (0.144)	Loss 0.8189 (0.7345)	Acc@1 80.000 (83.174)	Acc@5 95.000 (96.230)	Mem 1696MB
[2021-06-16 18:14:44 swin_small_patch4_window7_224] (main.py 274): INFO  * Acc@1 83.174 Acc@5 96.230
[2021-06-16 18:14:44 swin_small_patch4_window7_224] (main.py 123): INFO Accuracy of the network on the 50000 test images: 83.2%
[2021-06-17 09:39:38 swin_small_patch4_window7_224] (main.py 342): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2021-06-17 09:39:38 swin_small_patch4_window7_224] (main.py 345): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /workspace/mnt/storage/yankai/imagenet/ImageNet-pytorch
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2021-06-17 09:39:42 swin_small_patch4_window7_224] (main.py 79): INFO Creating model:swin/swin_small_patch4_window7_224
[2021-06-17 09:39:43 swin_small_patch4_window7_224] (main.py 82): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2021-06-17 09:39:43 swin_small_patch4_window7_224] (main.py 91): INFO number of params: 49606258
[2021-06-17 09:39:43 swin_small_patch4_window7_224] (main.py 94): INFO number of GFLOPs: 8.746520064
[2021-06-17 09:39:43 swin_small_patch4_window7_224] (main.py 118): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2021-06-17 09:39:43 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth....................
[2021-06-17 09:39:43 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-06-17 09:39:45 swin_small_patch4_window7_224] (main.py 268): INFO Test: [0/391]	Time 1.529 (1.529)	Loss 0.6338 (0.6338)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)	Mem 1596MB
[2021-06-17 09:39:46 swin_small_patch4_window7_224] (main.py 268): INFO Test: [10/391]	Time 0.141 (0.280)	Loss 0.8448 (0.7586)	Acc@1 81.250 (83.594)	Acc@5 94.531 (95.384)	Mem 1696MB
[2021-06-17 09:39:48 swin_small_patch4_window7_224] (main.py 268): INFO Test: [20/391]	Time 0.139 (0.213)	Loss 0.5611 (0.7336)	Acc@1 87.500 (83.705)	Acc@5 98.438 (95.908)	Mem 1696MB
[2021-06-17 09:39:49 swin_small_patch4_window7_224] (main.py 268): INFO Test: [30/391]	Time 0.137 (0.189)	Loss 0.8017 (0.7323)	Acc@1 84.375 (83.745)	Acc@5 94.531 (95.842)	Mem 1696MB
[2021-06-17 09:39:50 swin_small_patch4_window7_224] (main.py 268): INFO Test: [40/391]	Time 0.137 (0.176)	Loss 0.5868 (0.7513)	Acc@1 87.500 (83.213)	Acc@5 97.656 (95.903)	Mem 1696MB
[2021-06-17 09:39:52 swin_small_patch4_window7_224] (main.py 268): INFO Test: [50/391]	Time 0.137 (0.169)	Loss 0.8524 (0.7511)	Acc@1 81.250 (83.349)	Acc@5 93.750 (95.910)	Mem 1696MB
[2021-06-17 09:39:53 swin_small_patch4_window7_224] (main.py 268): INFO Test: [60/391]	Time 0.137 (0.164)	Loss 0.6387 (0.7528)	Acc@1 86.719 (83.363)	Acc@5 97.656 (95.889)	Mem 1696MB
[2021-06-17 09:39:54 swin_small_patch4_window7_224] (main.py 268): INFO Test: [70/391]	Time 0.137 (0.160)	Loss 0.7187 (0.7533)	Acc@1 82.031 (83.143)	Acc@5 95.312 (95.885)	Mem 1696MB
[2021-06-17 09:39:56 swin_small_patch4_window7_224] (main.py 268): INFO Test: [80/391]	Time 0.137 (0.157)	Loss 0.6027 (0.7440)	Acc@1 82.812 (83.266)	Acc@5 98.438 (96.017)	Mem 1696MB
[2021-06-17 09:39:57 swin_small_patch4_window7_224] (main.py 268): INFO Test: [90/391]	Time 0.140 (0.155)	Loss 0.7577 (0.7444)	Acc@1 82.812 (83.199)	Acc@5 95.312 (96.042)	Mem 1696MB
[2021-06-17 09:39:59 swin_small_patch4_window7_224] (main.py 268): INFO Test: [100/391]	Time 0.138 (0.154)	Loss 0.7772 (0.7400)	Acc@1 82.812 (83.331)	Acc@5 94.531 (96.078)	Mem 1696MB
[2021-06-17 09:40:00 swin_small_patch4_window7_224] (main.py 268): INFO Test: [110/391]	Time 0.137 (0.152)	Loss 0.6495 (0.7391)	Acc@1 85.938 (83.277)	Acc@5 97.656 (96.094)	Mem 1696MB
[2021-06-17 09:40:01 swin_small_patch4_window7_224] (main.py 268): INFO Test: [120/391]	Time 0.137 (0.151)	Loss 0.6951 (0.7398)	Acc@1 84.375 (83.187)	Acc@5 99.219 (96.126)	Mem 1696MB
[2021-06-17 09:40:03 swin_small_patch4_window7_224] (main.py 268): INFO Test: [130/391]	Time 0.137 (0.150)	Loss 0.6170 (0.7366)	Acc@1 85.156 (83.224)	Acc@5 98.438 (96.189)	Mem 1696MB
[2021-06-17 09:40:04 swin_small_patch4_window7_224] (main.py 268): INFO Test: [140/391]	Time 0.140 (0.149)	Loss 0.5930 (0.7395)	Acc@1 85.156 (83.211)	Acc@5 96.875 (96.160)	Mem 1696MB
[2021-06-17 09:40:06 swin_small_patch4_window7_224] (main.py 268): INFO Test: [150/391]	Time 0.138 (0.149)	Loss 0.6390 (0.7390)	Acc@1 85.938 (83.257)	Acc@5 96.094 (96.120)	Mem 1696MB
[2021-06-17 09:40:07 swin_small_patch4_window7_224] (main.py 268): INFO Test: [160/391]	Time 0.140 (0.149)	Loss 0.7642 (0.7395)	Acc@1 85.156 (83.269)	Acc@5 93.750 (96.108)	Mem 1696MB
[2021-06-17 09:40:08 swin_small_patch4_window7_224] (main.py 268): INFO Test: [170/391]	Time 0.138 (0.148)	Loss 0.7483 (0.7353)	Acc@1 84.375 (83.356)	Acc@5 94.531 (96.117)	Mem 1696MB
[2021-06-17 09:40:10 swin_small_patch4_window7_224] (main.py 268): INFO Test: [180/391]	Time 0.138 (0.148)	Loss 0.7400 (0.7379)	Acc@1 80.469 (83.292)	Acc@5 96.094 (96.107)	Mem 1696MB
[2021-06-17 09:40:11 swin_small_patch4_window7_224] (main.py 268): INFO Test: [190/391]	Time 0.138 (0.147)	Loss 0.7306 (0.7357)	Acc@1 77.344 (83.279)	Acc@5 97.656 (96.122)	Mem 1696MB
[2021-06-17 09:40:13 swin_small_patch4_window7_224] (main.py 268): INFO Test: [200/391]	Time 0.138 (0.147)	Loss 0.6897 (0.7322)	Acc@1 82.031 (83.329)	Acc@5 96.094 (96.137)	Mem 1696MB
[2021-06-17 09:40:14 swin_small_patch4_window7_224] (main.py 268): INFO Test: [210/391]	Time 0.139 (0.146)	Loss 0.7511 (0.7322)	Acc@1 83.594 (83.357)	Acc@5 95.312 (96.127)	Mem 1696MB
[2021-06-17 09:40:15 swin_small_patch4_window7_224] (main.py 268): INFO Test: [220/391]	Time 0.138 (0.146)	Loss 0.5003 (0.7313)	Acc@1 86.719 (83.368)	Acc@5 100.000 (96.154)	Mem 1696MB
[2021-06-17 09:40:17 swin_small_patch4_window7_224] (main.py 268): INFO Test: [230/391]	Time 0.138 (0.146)	Loss 0.8885 (0.7337)	Acc@1 78.906 (83.262)	Acc@5 93.750 (96.128)	Mem 1696MB
[2021-06-17 09:40:18 swin_small_patch4_window7_224] (main.py 268): INFO Test: [240/391]	Time 0.138 (0.146)	Loss 0.8530 (0.7391)	Acc@1 82.812 (83.198)	Acc@5 93.750 (96.087)	Mem 1696MB
[2021-06-17 09:40:20 swin_small_patch4_window7_224] (main.py 268): INFO Test: [250/391]	Time 0.139 (0.145)	Loss 0.6780 (0.7379)	Acc@1 85.938 (83.195)	Acc@5 96.094 (96.106)	Mem 1696MB
[2021-06-17 09:40:21 swin_small_patch4_window7_224] (main.py 268): INFO Test: [260/391]	Time 0.138 (0.145)	Loss 0.6471 (0.7363)	Acc@1 83.594 (83.214)	Acc@5 97.656 (96.142)	Mem 1696MB
[2021-06-17 09:40:22 swin_small_patch4_window7_224] (main.py 268): INFO Test: [270/391]	Time 0.139 (0.145)	Loss 0.5228 (0.7338)	Acc@1 88.281 (83.251)	Acc@5 96.875 (96.163)	Mem 1696MB
[2021-06-17 09:40:24 swin_small_patch4_window7_224] (main.py 268): INFO Test: [280/391]	Time 0.139 (0.145)	Loss 0.8027 (0.7350)	Acc@1 83.594 (83.246)	Acc@5 93.750 (96.138)	Mem 1696MB
[2021-06-17 09:40:25 swin_small_patch4_window7_224] (main.py 268): INFO Test: [290/391]	Time 0.139 (0.145)	Loss 0.6097 (0.7355)	Acc@1 87.500 (83.226)	Acc@5 97.656 (96.145)	Mem 1696MB
[2021-06-17 09:40:27 swin_small_patch4_window7_224] (main.py 268): INFO Test: [300/391]	Time 0.139 (0.144)	Loss 0.7353 (0.7346)	Acc@1 82.031 (83.241)	Acc@5 96.875 (96.156)	Mem 1696MB
[2021-06-17 09:40:28 swin_small_patch4_window7_224] (main.py 268): INFO Test: [310/391]	Time 0.141 (0.144)	Loss 0.5475 (0.7339)	Acc@1 86.719 (83.245)	Acc@5 96.875 (96.154)	Mem 1696MB
[2021-06-17 09:40:29 swin_small_patch4_window7_224] (main.py 268): INFO Test: [320/391]	Time 0.143 (0.144)	Loss 0.7023 (0.7344)	Acc@1 84.375 (83.241)	Acc@5 96.875 (96.147)	Mem 1696MB
[2021-06-17 09:40:31 swin_small_patch4_window7_224] (main.py 268): INFO Test: [330/391]	Time 0.139 (0.144)	Loss 0.6464 (0.7326)	Acc@1 81.250 (83.261)	Acc@5 97.656 (96.181)	Mem 1696MB
[2021-06-17 09:40:32 swin_small_patch4_window7_224] (main.py 268): INFO Test: [340/391]	Time 0.139 (0.144)	Loss 0.6625 (0.7325)	Acc@1 84.375 (83.268)	Acc@5 96.875 (96.188)	Mem 1696MB
[2021-06-17 09:40:34 swin_small_patch4_window7_224] (main.py 268): INFO Test: [350/391]	Time 0.139 (0.144)	Loss 0.5204 (0.7320)	Acc@1 88.281 (83.262)	Acc@5 97.656 (96.194)	Mem 1696MB
[2021-06-17 09:40:35 swin_small_patch4_window7_224] (main.py 268): INFO Test: [360/391]	Time 0.142 (0.144)	Loss 0.8791 (0.7327)	Acc@1 80.469 (83.224)	Acc@5 96.875 (96.198)	Mem 1696MB
[2021-06-17 09:40:36 swin_small_patch4_window7_224] (main.py 268): INFO Test: [370/391]	Time 0.143 (0.144)	Loss 0.6847 (0.7328)	Acc@1 87.500 (83.225)	Acc@5 98.438 (96.231)	Mem 1696MB
[2021-06-17 09:40:38 swin_small_patch4_window7_224] (main.py 268): INFO Test: [380/391]	Time 0.142 (0.144)	Loss 0.7560 (0.7343)	Acc@1 85.156 (83.208)	Acc@5 95.312 (96.231)	Mem 1696MB
[2021-06-17 09:40:39 swin_small_patch4_window7_224] (main.py 268): INFO Test: [390/391]	Time 0.109 (0.143)	Loss 0.8189 (0.7345)	Acc@1 80.000 (83.174)	Acc@5 95.000 (96.230)	Mem 1696MB
[2021-06-17 09:40:39 swin_small_patch4_window7_224] (main.py 274): INFO  * Acc@1 83.174 Acc@5 96.230
[2021-06-17 09:40:39 swin_small_patch4_window7_224] (main.py 123): INFO Accuracy of the network on the 50000 test images: 83.2%
[2021-06-17 16:39:47 swin_small_patch4_window7_224] (main.py 342): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2021-06-17 16:39:47 swin_small_patch4_window7_224] (main.py 345): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /workspace/mnt/storage/yankai/imagenet/ImageNet-pytorch
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2021-06-17 16:39:51 swin_small_patch4_window7_224] (main.py 79): INFO Creating model:swin/swin_small_patch4_window7_224
[2021-06-17 16:39:52 swin_small_patch4_window7_224] (main.py 82): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2021-06-17 16:39:52 swin_small_patch4_window7_224] (main.py 91): INFO number of params: 49606258
[2021-06-17 16:39:52 swin_small_patch4_window7_224] (main.py 94): INFO number of GFLOPs: 8.746520064
[2021-06-17 16:39:52 swin_small_patch4_window7_224] (main.py 118): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2021-06-17 16:39:52 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth....................
[2021-06-17 16:39:52 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-06-17 16:39:54 swin_small_patch4_window7_224] (main.py 268): INFO Test: [0/391]	Time 1.839 (1.839)	Loss 0.6338 (0.6338)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)	Mem 1596MB
[2021-06-17 16:39:56 swin_small_patch4_window7_224] (main.py 268): INFO Test: [10/391]	Time 0.141 (0.305)	Loss 0.8448 (0.7586)	Acc@1 81.250 (83.594)	Acc@5 94.531 (95.384)	Mem 1696MB
[2021-06-17 16:39:57 swin_small_patch4_window7_224] (main.py 268): INFO Test: [20/391]	Time 0.142 (0.228)	Loss 0.5611 (0.7336)	Acc@1 87.500 (83.705)	Acc@5 98.438 (95.908)	Mem 1696MB
[2021-06-17 16:39:58 swin_small_patch4_window7_224] (main.py 268): INFO Test: [30/391]	Time 0.139 (0.199)	Loss 0.8017 (0.7323)	Acc@1 84.375 (83.745)	Acc@5 94.531 (95.842)	Mem 1696MB
[2021-06-17 16:40:00 swin_small_patch4_window7_224] (main.py 268): INFO Test: [40/391]	Time 0.138 (0.184)	Loss 0.5868 (0.7513)	Acc@1 87.500 (83.213)	Acc@5 97.656 (95.903)	Mem 1696MB
[2021-06-17 16:40:01 swin_small_patch4_window7_224] (main.py 268): INFO Test: [50/391]	Time 0.138 (0.176)	Loss 0.8524 (0.7511)	Acc@1 81.250 (83.349)	Acc@5 93.750 (95.910)	Mem 1696MB
[2021-06-17 16:40:03 swin_small_patch4_window7_224] (main.py 268): INFO Test: [60/391]	Time 0.137 (0.169)	Loss 0.6387 (0.7528)	Acc@1 86.719 (83.363)	Acc@5 97.656 (95.889)	Mem 1696MB
[2021-06-17 16:40:04 swin_small_patch4_window7_224] (main.py 268): INFO Test: [70/391]	Time 0.137 (0.165)	Loss 0.7187 (0.7533)	Acc@1 82.031 (83.143)	Acc@5 95.312 (95.885)	Mem 1696MB
[2021-06-17 16:40:05 swin_small_patch4_window7_224] (main.py 268): INFO Test: [80/391]	Time 0.138 (0.162)	Loss 0.6027 (0.7440)	Acc@1 82.812 (83.266)	Acc@5 98.438 (96.017)	Mem 1696MB
[2021-06-17 16:40:07 swin_small_patch4_window7_224] (main.py 268): INFO Test: [90/391]	Time 0.142 (0.159)	Loss 0.7577 (0.7444)	Acc@1 82.812 (83.199)	Acc@5 95.312 (96.042)	Mem 1696MB
[2021-06-17 16:40:08 swin_small_patch4_window7_224] (main.py 268): INFO Test: [100/391]	Time 0.138 (0.157)	Loss 0.7772 (0.7400)	Acc@1 82.812 (83.331)	Acc@5 94.531 (96.078)	Mem 1696MB
[2021-06-17 16:40:10 swin_small_patch4_window7_224] (main.py 268): INFO Test: [110/391]	Time 0.138 (0.156)	Loss 0.6495 (0.7391)	Acc@1 85.938 (83.277)	Acc@5 97.656 (96.094)	Mem 1696MB
[2021-06-17 16:40:11 swin_small_patch4_window7_224] (main.py 268): INFO Test: [120/391]	Time 0.138 (0.154)	Loss 0.6951 (0.7398)	Acc@1 84.375 (83.187)	Acc@5 99.219 (96.126)	Mem 1696MB
[2021-06-17 16:40:12 swin_small_patch4_window7_224] (main.py 268): INFO Test: [130/391]	Time 0.139 (0.153)	Loss 0.6170 (0.7366)	Acc@1 85.156 (83.224)	Acc@5 98.438 (96.189)	Mem 1696MB
[2021-06-17 16:40:14 swin_small_patch4_window7_224] (main.py 268): INFO Test: [140/391]	Time 0.138 (0.152)	Loss 0.5930 (0.7395)	Acc@1 85.156 (83.211)	Acc@5 96.875 (96.160)	Mem 1696MB
[2021-06-17 16:40:15 swin_small_patch4_window7_224] (main.py 268): INFO Test: [150/391]	Time 0.139 (0.151)	Loss 0.6390 (0.7390)	Acc@1 85.938 (83.257)	Acc@5 96.094 (96.120)	Mem 1696MB
[2021-06-17 16:40:17 swin_small_patch4_window7_224] (main.py 268): INFO Test: [160/391]	Time 0.139 (0.152)	Loss 0.7642 (0.7395)	Acc@1 85.156 (83.269)	Acc@5 93.750 (96.108)	Mem 1696MB
[2021-06-17 16:40:18 swin_small_patch4_window7_224] (main.py 268): INFO Test: [170/391]	Time 0.138 (0.151)	Loss 0.7483 (0.7353)	Acc@1 84.375 (83.356)	Acc@5 94.531 (96.117)	Mem 1696MB
[2021-06-17 16:40:19 swin_small_patch4_window7_224] (main.py 268): INFO Test: [180/391]	Time 0.139 (0.150)	Loss 0.7400 (0.7379)	Acc@1 80.469 (83.292)	Acc@5 96.094 (96.107)	Mem 1696MB
[2021-06-17 16:40:21 swin_small_patch4_window7_224] (main.py 268): INFO Test: [190/391]	Time 0.137 (0.150)	Loss 0.7306 (0.7357)	Acc@1 77.344 (83.279)	Acc@5 97.656 (96.122)	Mem 1696MB
[2021-06-17 16:40:22 swin_small_patch4_window7_224] (main.py 268): INFO Test: [200/391]	Time 0.139 (0.149)	Loss 0.6897 (0.7322)	Acc@1 82.031 (83.329)	Acc@5 96.094 (96.137)	Mem 1696MB
[2021-06-17 16:40:24 swin_small_patch4_window7_224] (main.py 268): INFO Test: [210/391]	Time 0.142 (0.149)	Loss 0.7511 (0.7322)	Acc@1 83.594 (83.357)	Acc@5 95.312 (96.127)	Mem 1696MB
[2021-06-17 16:40:25 swin_small_patch4_window7_224] (main.py 268): INFO Test: [220/391]	Time 0.139 (0.148)	Loss 0.5003 (0.7313)	Acc@1 86.719 (83.368)	Acc@5 100.000 (96.154)	Mem 1696MB
[2021-06-17 16:40:26 swin_small_patch4_window7_224] (main.py 268): INFO Test: [230/391]	Time 0.139 (0.148)	Loss 0.8885 (0.7337)	Acc@1 78.906 (83.262)	Acc@5 93.750 (96.128)	Mem 1696MB
[2021-06-17 16:40:28 swin_small_patch4_window7_224] (main.py 268): INFO Test: [240/391]	Time 0.140 (0.148)	Loss 0.8530 (0.7391)	Acc@1 82.812 (83.198)	Acc@5 93.750 (96.087)	Mem 1696MB
[2021-06-17 16:40:29 swin_small_patch4_window7_224] (main.py 268): INFO Test: [250/391]	Time 0.138 (0.147)	Loss 0.6780 (0.7379)	Acc@1 85.938 (83.195)	Acc@5 96.094 (96.106)	Mem 1696MB
[2021-06-17 16:40:31 swin_small_patch4_window7_224] (main.py 268): INFO Test: [260/391]	Time 0.139 (0.147)	Loss 0.6471 (0.7363)	Acc@1 83.594 (83.214)	Acc@5 97.656 (96.142)	Mem 1696MB
[2021-06-17 16:40:32 swin_small_patch4_window7_224] (main.py 268): INFO Test: [270/391]	Time 0.140 (0.147)	Loss 0.5228 (0.7338)	Acc@1 88.281 (83.251)	Acc@5 96.875 (96.163)	Mem 1696MB
[2021-06-17 16:40:33 swin_small_patch4_window7_224] (main.py 268): INFO Test: [280/391]	Time 0.146 (0.146)	Loss 0.8027 (0.7350)	Acc@1 83.594 (83.246)	Acc@5 93.750 (96.138)	Mem 1696MB
[2021-06-17 16:40:35 swin_small_patch4_window7_224] (main.py 268): INFO Test: [290/391]	Time 0.140 (0.146)	Loss 0.6097 (0.7355)	Acc@1 87.500 (83.226)	Acc@5 97.656 (96.145)	Mem 1696MB
[2021-06-17 16:40:36 swin_small_patch4_window7_224] (main.py 268): INFO Test: [300/391]	Time 0.139 (0.146)	Loss 0.7353 (0.7346)	Acc@1 82.031 (83.241)	Acc@5 96.875 (96.156)	Mem 1696MB
[2021-06-17 16:40:38 swin_small_patch4_window7_224] (main.py 268): INFO Test: [310/391]	Time 0.139 (0.146)	Loss 0.5475 (0.7339)	Acc@1 86.719 (83.245)	Acc@5 96.875 (96.154)	Mem 1696MB
[2021-06-17 16:40:39 swin_small_patch4_window7_224] (main.py 268): INFO Test: [320/391]	Time 0.140 (0.146)	Loss 0.7023 (0.7344)	Acc@1 84.375 (83.241)	Acc@5 96.875 (96.147)	Mem 1696MB
[2021-06-17 16:40:40 swin_small_patch4_window7_224] (main.py 268): INFO Test: [330/391]	Time 0.139 (0.146)	Loss 0.6464 (0.7326)	Acc@1 81.250 (83.261)	Acc@5 97.656 (96.181)	Mem 1696MB
[2021-06-17 16:40:42 swin_small_patch4_window7_224] (main.py 268): INFO Test: [340/391]	Time 0.139 (0.145)	Loss 0.6625 (0.7325)	Acc@1 84.375 (83.268)	Acc@5 96.875 (96.188)	Mem 1696MB
[2021-06-17 16:40:43 swin_small_patch4_window7_224] (main.py 268): INFO Test: [350/391]	Time 0.139 (0.145)	Loss 0.5204 (0.7320)	Acc@1 88.281 (83.262)	Acc@5 97.656 (96.194)	Mem 1696MB
[2021-06-17 16:40:45 swin_small_patch4_window7_224] (main.py 268): INFO Test: [360/391]	Time 0.140 (0.145)	Loss 0.8791 (0.7327)	Acc@1 80.469 (83.224)	Acc@5 96.875 (96.198)	Mem 1696MB
[2021-06-17 16:40:46 swin_small_patch4_window7_224] (main.py 268): INFO Test: [370/391]	Time 0.142 (0.145)	Loss 0.6847 (0.7328)	Acc@1 87.500 (83.225)	Acc@5 98.438 (96.231)	Mem 1696MB
[2021-06-17 16:40:47 swin_small_patch4_window7_224] (main.py 268): INFO Test: [380/391]	Time 0.148 (0.145)	Loss 0.7560 (0.7343)	Acc@1 85.156 (83.208)	Acc@5 95.312 (96.231)	Mem 1696MB
[2021-06-17 16:40:49 swin_small_patch4_window7_224] (main.py 268): INFO Test: [390/391]	Time 0.129 (0.145)	Loss 0.8189 (0.7345)	Acc@1 80.000 (83.174)	Acc@5 95.000 (96.230)	Mem 1696MB
[2021-06-17 16:40:49 swin_small_patch4_window7_224] (main.py 274): INFO  * Acc@1 83.174 Acc@5 96.230
[2021-06-17 16:40:49 swin_small_patch4_window7_224] (main.py 123): INFO Accuracy of the network on the 50000 test images: 83.2%
[2021-06-18 09:45:17 swin_small_patch4_window7_224] (main.py 342): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2021-06-18 09:45:17 swin_small_patch4_window7_224] (main.py 345): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /workspace/mnt/storage/yankai/imagenet/ImageNet-pytorch
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2021-06-18 09:45:21 swin_small_patch4_window7_224] (main.py 79): INFO Creating model:swin/swin_small_patch4_window7_224
[2021-06-18 09:45:21 swin_small_patch4_window7_224] (main.py 82): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2021-06-18 09:45:21 swin_small_patch4_window7_224] (main.py 91): INFO number of params: 49606258
[2021-06-18 09:45:21 swin_small_patch4_window7_224] (main.py 94): INFO number of GFLOPs: 8.746520064
[2021-06-18 09:45:21 swin_small_patch4_window7_224] (main.py 118): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2021-06-18 09:45:21 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth....................
[2021-06-18 09:45:22 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-06-18 09:45:23 swin_small_patch4_window7_224] (main.py 268): INFO Test: [0/391]	Time 1.393 (1.393)	Loss 0.6338 (0.6338)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)	Mem 1596MB
[2021-06-18 09:45:25 swin_small_patch4_window7_224] (main.py 268): INFO Test: [10/391]	Time 0.136 (0.274)	Loss 0.8448 (0.7586)	Acc@1 81.250 (83.594)	Acc@5 94.531 (95.384)	Mem 1696MB
[2021-06-18 09:45:26 swin_small_patch4_window7_224] (main.py 268): INFO Test: [20/391]	Time 0.137 (0.209)	Loss 0.5611 (0.7336)	Acc@1 87.500 (83.705)	Acc@5 98.438 (95.908)	Mem 1696MB
[2021-06-18 09:45:28 swin_small_patch4_window7_224] (main.py 268): INFO Test: [30/391]	Time 0.136 (0.186)	Loss 0.8017 (0.7323)	Acc@1 84.375 (83.745)	Acc@5 94.531 (95.842)	Mem 1696MB
[2021-06-18 09:45:29 swin_small_patch4_window7_224] (main.py 268): INFO Test: [40/391]	Time 0.137 (0.174)	Loss 0.5868 (0.7513)	Acc@1 87.500 (83.213)	Acc@5 97.656 (95.903)	Mem 1696MB
[2021-06-18 09:45:30 swin_small_patch4_window7_224] (main.py 268): INFO Test: [50/391]	Time 0.137 (0.167)	Loss 0.8524 (0.7511)	Acc@1 81.250 (83.349)	Acc@5 93.750 (95.910)	Mem 1696MB
[2021-06-18 09:45:32 swin_small_patch4_window7_224] (main.py 268): INFO Test: [60/391]	Time 0.137 (0.162)	Loss 0.6387 (0.7528)	Acc@1 86.719 (83.363)	Acc@5 97.656 (95.889)	Mem 1696MB
[2021-06-18 09:45:33 swin_small_patch4_window7_224] (main.py 268): INFO Test: [70/391]	Time 0.137 (0.159)	Loss 0.7187 (0.7533)	Acc@1 82.031 (83.143)	Acc@5 95.312 (95.885)	Mem 1696MB
[2021-06-18 09:45:34 swin_small_patch4_window7_224] (main.py 268): INFO Test: [80/391]	Time 0.141 (0.156)	Loss 0.6027 (0.7440)	Acc@1 82.812 (83.266)	Acc@5 98.438 (96.017)	Mem 1696MB
[2021-06-18 09:45:36 swin_small_patch4_window7_224] (main.py 268): INFO Test: [90/391]	Time 0.137 (0.154)	Loss 0.7577 (0.7444)	Acc@1 82.812 (83.199)	Acc@5 95.312 (96.042)	Mem 1696MB
[2021-06-18 09:45:37 swin_small_patch4_window7_224] (main.py 268): INFO Test: [100/391]	Time 0.143 (0.152)	Loss 0.7772 (0.7400)	Acc@1 82.812 (83.331)	Acc@5 94.531 (96.078)	Mem 1696MB
[2021-06-18 09:45:39 swin_small_patch4_window7_224] (main.py 268): INFO Test: [110/391]	Time 0.141 (0.151)	Loss 0.6495 (0.7391)	Acc@1 85.938 (83.277)	Acc@5 97.656 (96.094)	Mem 1696MB
[2021-06-18 09:45:40 swin_small_patch4_window7_224] (main.py 268): INFO Test: [120/391]	Time 0.137 (0.150)	Loss 0.6951 (0.7398)	Acc@1 84.375 (83.187)	Acc@5 99.219 (96.126)	Mem 1696MB
[2021-06-18 09:45:41 swin_small_patch4_window7_224] (main.py 268): INFO Test: [130/391]	Time 0.137 (0.149)	Loss 0.6170 (0.7366)	Acc@1 85.156 (83.224)	Acc@5 98.438 (96.189)	Mem 1696MB
[2021-06-18 09:45:43 swin_small_patch4_window7_224] (main.py 268): INFO Test: [140/391]	Time 0.137 (0.148)	Loss 0.5930 (0.7395)	Acc@1 85.156 (83.211)	Acc@5 96.875 (96.160)	Mem 1696MB
[2021-06-18 09:45:44 swin_small_patch4_window7_224] (main.py 268): INFO Test: [150/391]	Time 0.138 (0.147)	Loss 0.6390 (0.7390)	Acc@1 85.938 (83.257)	Acc@5 96.094 (96.120)	Mem 1696MB
[2021-06-18 09:45:45 swin_small_patch4_window7_224] (main.py 268): INFO Test: [160/391]	Time 0.138 (0.147)	Loss 0.7642 (0.7395)	Acc@1 85.156 (83.269)	Acc@5 93.750 (96.108)	Mem 1696MB
[2021-06-18 09:45:47 swin_small_patch4_window7_224] (main.py 268): INFO Test: [170/391]	Time 0.138 (0.147)	Loss 0.7483 (0.7353)	Acc@1 84.375 (83.356)	Acc@5 94.531 (96.117)	Mem 1696MB
[2021-06-18 09:45:48 swin_small_patch4_window7_224] (main.py 268): INFO Test: [180/391]	Time 0.138 (0.146)	Loss 0.7400 (0.7379)	Acc@1 80.469 (83.292)	Acc@5 96.094 (96.107)	Mem 1696MB
[2021-06-18 09:45:50 swin_small_patch4_window7_224] (main.py 268): INFO Test: [190/391]	Time 0.138 (0.146)	Loss 0.7306 (0.7357)	Acc@1 77.344 (83.279)	Acc@5 97.656 (96.122)	Mem 1696MB
[2021-06-18 09:45:51 swin_small_patch4_window7_224] (main.py 268): INFO Test: [200/391]	Time 0.138 (0.146)	Loss 0.6897 (0.7322)	Acc@1 82.031 (83.329)	Acc@5 96.094 (96.137)	Mem 1696MB
[2021-06-18 09:45:52 swin_small_patch4_window7_224] (main.py 268): INFO Test: [210/391]	Time 0.138 (0.145)	Loss 0.7511 (0.7322)	Acc@1 83.594 (83.357)	Acc@5 95.312 (96.127)	Mem 1696MB
[2021-06-18 09:45:54 swin_small_patch4_window7_224] (main.py 268): INFO Test: [220/391]	Time 0.138 (0.145)	Loss 0.5003 (0.7313)	Acc@1 86.719 (83.368)	Acc@5 100.000 (96.154)	Mem 1696MB
[2021-06-18 09:45:55 swin_small_patch4_window7_224] (main.py 268): INFO Test: [230/391]	Time 0.138 (0.145)	Loss 0.8885 (0.7337)	Acc@1 78.906 (83.262)	Acc@5 93.750 (96.128)	Mem 1696MB
[2021-06-18 09:45:57 swin_small_patch4_window7_224] (main.py 268): INFO Test: [240/391]	Time 0.138 (0.144)	Loss 0.8530 (0.7391)	Acc@1 82.812 (83.198)	Acc@5 93.750 (96.087)	Mem 1696MB
[2021-06-18 09:45:58 swin_small_patch4_window7_224] (main.py 268): INFO Test: [250/391]	Time 0.138 (0.144)	Loss 0.6780 (0.7379)	Acc@1 85.938 (83.195)	Acc@5 96.094 (96.106)	Mem 1696MB
[2021-06-18 09:45:59 swin_small_patch4_window7_224] (main.py 268): INFO Test: [260/391]	Time 0.139 (0.144)	Loss 0.6471 (0.7363)	Acc@1 83.594 (83.214)	Acc@5 97.656 (96.142)	Mem 1696MB
[2021-06-18 09:46:01 swin_small_patch4_window7_224] (main.py 268): INFO Test: [270/391]	Time 0.138 (0.144)	Loss 0.5228 (0.7338)	Acc@1 88.281 (83.251)	Acc@5 96.875 (96.163)	Mem 1696MB
[2021-06-18 09:46:02 swin_small_patch4_window7_224] (main.py 268): INFO Test: [280/391]	Time 0.138 (0.144)	Loss 0.8027 (0.7350)	Acc@1 83.594 (83.246)	Acc@5 93.750 (96.138)	Mem 1696MB
[2021-06-18 09:46:03 swin_small_patch4_window7_224] (main.py 268): INFO Test: [290/391]	Time 0.139 (0.143)	Loss 0.6097 (0.7355)	Acc@1 87.500 (83.226)	Acc@5 97.656 (96.145)	Mem 1696MB
[2021-06-18 09:46:05 swin_small_patch4_window7_224] (main.py 268): INFO Test: [300/391]	Time 0.139 (0.143)	Loss 0.7353 (0.7346)	Acc@1 82.031 (83.241)	Acc@5 96.875 (96.156)	Mem 1696MB
[2021-06-18 09:46:06 swin_small_patch4_window7_224] (main.py 268): INFO Test: [310/391]	Time 0.139 (0.143)	Loss 0.5475 (0.7339)	Acc@1 86.719 (83.245)	Acc@5 96.875 (96.154)	Mem 1696MB
[2021-06-18 09:46:08 swin_small_patch4_window7_224] (main.py 268): INFO Test: [320/391]	Time 0.139 (0.143)	Loss 0.7023 (0.7344)	Acc@1 84.375 (83.241)	Acc@5 96.875 (96.147)	Mem 1696MB
[2021-06-18 09:46:09 swin_small_patch4_window7_224] (main.py 268): INFO Test: [330/391]	Time 0.139 (0.143)	Loss 0.6464 (0.7326)	Acc@1 81.250 (83.261)	Acc@5 97.656 (96.181)	Mem 1696MB
[2021-06-18 09:46:10 swin_small_patch4_window7_224] (main.py 268): INFO Test: [340/391]	Time 0.139 (0.143)	Loss 0.6625 (0.7325)	Acc@1 84.375 (83.268)	Acc@5 96.875 (96.188)	Mem 1696MB
[2021-06-18 09:46:12 swin_small_patch4_window7_224] (main.py 268): INFO Test: [350/391]	Time 0.139 (0.143)	Loss 0.5204 (0.7320)	Acc@1 88.281 (83.262)	Acc@5 97.656 (96.194)	Mem 1696MB
[2021-06-18 09:46:13 swin_small_patch4_window7_224] (main.py 268): INFO Test: [360/391]	Time 0.139 (0.143)	Loss 0.8791 (0.7327)	Acc@1 80.469 (83.224)	Acc@5 96.875 (96.198)	Mem 1696MB
[2021-06-18 09:46:15 swin_small_patch4_window7_224] (main.py 268): INFO Test: [370/391]	Time 0.139 (0.142)	Loss 0.6847 (0.7328)	Acc@1 87.500 (83.225)	Acc@5 98.438 (96.231)	Mem 1696MB
[2021-06-18 09:46:16 swin_small_patch4_window7_224] (main.py 268): INFO Test: [380/391]	Time 0.139 (0.142)	Loss 0.7560 (0.7343)	Acc@1 85.156 (83.208)	Acc@5 95.312 (96.231)	Mem 1696MB
[2021-06-18 09:46:17 swin_small_patch4_window7_224] (main.py 268): INFO Test: [390/391]	Time 0.118 (0.142)	Loss 0.8189 (0.7345)	Acc@1 80.000 (83.174)	Acc@5 95.000 (96.230)	Mem 1696MB
[2021-06-18 09:46:17 swin_small_patch4_window7_224] (main.py 274): INFO  * Acc@1 83.174 Acc@5 96.230
[2021-06-18 09:46:17 swin_small_patch4_window7_224] (main.py 123): INFO Accuracy of the network on the 50000 test images: 83.2%
[2021-06-18 16:18:37 swin_small_patch4_window7_224] (main.py 342): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2021-06-18 16:18:37 swin_small_patch4_window7_224] (main.py 345): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /workspace/mnt/storage/yankai/imagenet/ImageNet-pytorch
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  RESUME: /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2021-06-18 16:18:41 swin_small_patch4_window7_224] (main.py 79): INFO Creating model:swin/swin_small_patch4_window7_224
[2021-06-18 16:18:42 swin_small_patch4_window7_224] (main.py 82): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2021-06-18 16:18:42 swin_small_patch4_window7_224] (main.py 91): INFO number of params: 49606258
[2021-06-18 16:18:42 swin_small_patch4_window7_224] (main.py 94): INFO number of GFLOPs: 8.746520064
[2021-06-18 16:18:42 swin_small_patch4_window7_224] (main.py 118): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2021-06-18 16:18:42 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /workspace/mnt/storage/kanghaidong/cloud_project/basic_files/swin_small_patch4_window7_224.pth....................
[2021-06-18 16:18:42 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2021-06-18 16:18:43 swin_small_patch4_window7_224] (main.py 268): INFO Test: [0/391]	Time 1.528 (1.528)	Loss 0.6338 (0.6338)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)	Mem 1596MB
[2021-06-18 16:18:45 swin_small_patch4_window7_224] (main.py 268): INFO Test: [10/391]	Time 0.139 (0.277)	Loss 0.8448 (0.7586)	Acc@1 81.250 (83.594)	Acc@5 94.531 (95.384)	Mem 1696MB
[2021-06-18 16:18:46 swin_small_patch4_window7_224] (main.py 268): INFO Test: [20/391]	Time 0.136 (0.212)	Loss 0.5611 (0.7336)	Acc@1 87.500 (83.705)	Acc@5 98.438 (95.908)	Mem 1696MB
[2021-06-18 16:18:48 swin_small_patch4_window7_224] (main.py 268): INFO Test: [30/391]	Time 0.137 (0.188)	Loss 0.8017 (0.7323)	Acc@1 84.375 (83.745)	Acc@5 94.531 (95.842)	Mem 1696MB
[2021-06-18 16:18:49 swin_small_patch4_window7_224] (main.py 268): INFO Test: [40/391]	Time 0.137 (0.175)	Loss 0.5868 (0.7513)	Acc@1 87.500 (83.213)	Acc@5 97.656 (95.903)	Mem 1696MB
[2021-06-18 16:18:50 swin_small_patch4_window7_224] (main.py 268): INFO Test: [50/391]	Time 0.137 (0.168)	Loss 0.8524 (0.7511)	Acc@1 81.250 (83.349)	Acc@5 93.750 (95.910)	Mem 1696MB
[2021-06-18 16:18:52 swin_small_patch4_window7_224] (main.py 268): INFO Test: [60/391]	Time 0.137 (0.163)	Loss 0.6387 (0.7528)	Acc@1 86.719 (83.363)	Acc@5 97.656 (95.889)	Mem 1696MB
[2021-06-18 16:18:53 swin_small_patch4_window7_224] (main.py 268): INFO Test: [70/391]	Time 0.137 (0.159)	Loss 0.7187 (0.7533)	Acc@1 82.031 (83.143)	Acc@5 95.312 (95.885)	Mem 1696MB
[2021-06-18 16:18:55 swin_small_patch4_window7_224] (main.py 268): INFO Test: [80/391]	Time 0.139 (0.156)	Loss 0.6027 (0.7440)	Acc@1 82.812 (83.266)	Acc@5 98.438 (96.017)	Mem 1696MB
[2021-06-18 16:18:56 swin_small_patch4_window7_224] (main.py 268): INFO Test: [90/391]	Time 0.137 (0.154)	Loss 0.7577 (0.7444)	Acc@1 82.812 (83.199)	Acc@5 95.312 (96.042)	Mem 1696MB
[2021-06-18 16:18:57 swin_small_patch4_window7_224] (main.py 268): INFO Test: [100/391]	Time 0.144 (0.153)	Loss 0.7772 (0.7400)	Acc@1 82.812 (83.331)	Acc@5 94.531 (96.078)	Mem 1696MB
[2021-06-18 16:18:59 swin_small_patch4_window7_224] (main.py 268): INFO Test: [110/391]	Time 0.137 (0.151)	Loss 0.6495 (0.7391)	Acc@1 85.938 (83.277)	Acc@5 97.656 (96.094)	Mem 1696MB
[2021-06-18 16:19:00 swin_small_patch4_window7_224] (main.py 268): INFO Test: [120/391]	Time 0.137 (0.150)	Loss 0.6951 (0.7398)	Acc@1 84.375 (83.187)	Acc@5 99.219 (96.126)	Mem 1696MB
[2021-06-18 16:19:01 swin_small_patch4_window7_224] (main.py 268): INFO Test: [130/391]	Time 0.137 (0.149)	Loss 0.6170 (0.7366)	Acc@1 85.156 (83.224)	Acc@5 98.438 (96.189)	Mem 1696MB
[2021-06-18 16:19:03 swin_small_patch4_window7_224] (main.py 268): INFO Test: [140/391]	Time 0.143 (0.149)	Loss 0.5930 (0.7395)	Acc@1 85.156 (83.211)	Acc@5 96.875 (96.160)	Mem 1696MB
[2021-06-18 16:19:04 swin_small_patch4_window7_224] (main.py 268): INFO Test: [150/391]	Time 0.138 (0.148)	Loss 0.6390 (0.7390)	Acc@1 85.938 (83.257)	Acc@5 96.094 (96.120)	Mem 1696MB
[2021-06-18 16:19:06 swin_small_patch4_window7_224] (main.py 268): INFO Test: [160/391]	Time 0.137 (0.148)	Loss 0.7642 (0.7395)	Acc@1 85.156 (83.269)	Acc@5 93.750 (96.108)	Mem 1696MB
[2021-06-18 16:19:07 swin_small_patch4_window7_224] (main.py 268): INFO Test: [170/391]	Time 0.138 (0.148)	Loss 0.7483 (0.7353)	Acc@1 84.375 (83.356)	Acc@5 94.531 (96.117)	Mem 1696MB
[2021-06-18 16:19:08 swin_small_patch4_window7_224] (main.py 268): INFO Test: [180/391]	Time 0.138 (0.147)	Loss 0.7400 (0.7379)	Acc@1 80.469 (83.292)	Acc@5 96.094 (96.107)	Mem 1696MB
[2021-06-18 16:19:10 swin_small_patch4_window7_224] (main.py 268): INFO Test: [190/391]	Time 0.138 (0.147)	Loss 0.7306 (0.7357)	Acc@1 77.344 (83.279)	Acc@5 97.656 (96.122)	Mem 1696MB
[2021-06-18 16:19:11 swin_small_patch4_window7_224] (main.py 268): INFO Test: [200/391]	Time 0.138 (0.146)	Loss 0.6897 (0.7322)	Acc@1 82.031 (83.329)	Acc@5 96.094 (96.137)	Mem 1696MB
[2021-06-18 16:19:13 swin_small_patch4_window7_224] (main.py 268): INFO Test: [210/391]	Time 0.138 (0.146)	Loss 0.7511 (0.7322)	Acc@1 83.594 (83.357)	Acc@5 95.312 (96.127)	Mem 1696MB
[2021-06-18 16:19:14 swin_small_patch4_window7_224] (main.py 268): INFO Test: [220/391]	Time 0.138 (0.145)	Loss 0.5003 (0.7313)	Acc@1 86.719 (83.368)	Acc@5 100.000 (96.154)	Mem 1696MB
[2021-06-18 16:19:15 swin_small_patch4_window7_224] (main.py 268): INFO Test: [230/391]	Time 0.138 (0.145)	Loss 0.8885 (0.7337)	Acc@1 78.906 (83.262)	Acc@5 93.750 (96.128)	Mem 1696MB
[2021-06-18 16:19:17 swin_small_patch4_window7_224] (main.py 268): INFO Test: [240/391]	Time 0.138 (0.145)	Loss 0.8530 (0.7391)	Acc@1 82.812 (83.198)	Acc@5 93.750 (96.087)	Mem 1696MB
[2021-06-18 16:19:18 swin_small_patch4_window7_224] (main.py 268): INFO Test: [250/391]	Time 0.138 (0.145)	Loss 0.6780 (0.7379)	Acc@1 85.938 (83.195)	Acc@5 96.094 (96.106)	Mem 1696MB
[2021-06-18 16:19:20 swin_small_patch4_window7_224] (main.py 268): INFO Test: [260/391]	Time 0.138 (0.144)	Loss 0.6471 (0.7363)	Acc@1 83.594 (83.214)	Acc@5 97.656 (96.142)	Mem 1696MB
[2021-06-18 16:19:21 swin_small_patch4_window7_224] (main.py 268): INFO Test: [270/391]	Time 0.139 (0.144)	Loss 0.5228 (0.7338)	Acc@1 88.281 (83.251)	Acc@5 96.875 (96.163)	Mem 1696MB
[2021-06-18 16:19:22 swin_small_patch4_window7_224] (main.py 268): INFO Test: [280/391]	Time 0.139 (0.144)	Loss 0.8027 (0.7350)	Acc@1 83.594 (83.246)	Acc@5 93.750 (96.138)	Mem 1696MB
[2021-06-18 16:19:24 swin_small_patch4_window7_224] (main.py 268): INFO Test: [290/391]	Time 0.138 (0.144)	Loss 0.6097 (0.7355)	Acc@1 87.500 (83.226)	Acc@5 97.656 (96.145)	Mem 1696MB
[2021-06-18 16:19:25 swin_small_patch4_window7_224] (main.py 268): INFO Test: [300/391]	Time 0.139 (0.144)	Loss 0.7353 (0.7346)	Acc@1 82.031 (83.241)	Acc@5 96.875 (96.156)	Mem 1696MB
[2021-06-18 16:19:26 swin_small_patch4_window7_224] (main.py 268): INFO Test: [310/391]	Time 0.139 (0.143)	Loss 0.5475 (0.7339)	Acc@1 86.719 (83.245)	Acc@5 96.875 (96.154)	Mem 1696MB
[2021-06-18 16:19:28 swin_small_patch4_window7_224] (main.py 268): INFO Test: [320/391]	Time 0.139 (0.143)	Loss 0.7023 (0.7344)	Acc@1 84.375 (83.241)	Acc@5 96.875 (96.147)	Mem 1696MB
[2021-06-18 16:19:29 swin_small_patch4_window7_224] (main.py 268): INFO Test: [330/391]	Time 0.139 (0.143)	Loss 0.6464 (0.7326)	Acc@1 81.250 (83.261)	Acc@5 97.656 (96.181)	Mem 1696MB
[2021-06-18 16:19:31 swin_small_patch4_window7_224] (main.py 268): INFO Test: [340/391]	Time 0.139 (0.143)	Loss 0.6625 (0.7325)	Acc@1 84.375 (83.268)	Acc@5 96.875 (96.188)	Mem 1696MB
[2021-06-18 16:19:32 swin_small_patch4_window7_224] (main.py 268): INFO Test: [350/391]	Time 0.139 (0.143)	Loss 0.5204 (0.7320)	Acc@1 88.281 (83.262)	Acc@5 97.656 (96.194)	Mem 1696MB
[2021-06-18 16:19:33 swin_small_patch4_window7_224] (main.py 268): INFO Test: [360/391]	Time 0.142 (0.143)	Loss 0.8791 (0.7327)	Acc@1 80.469 (83.224)	Acc@5 96.875 (96.198)	Mem 1696MB
[2021-06-18 16:19:35 swin_small_patch4_window7_224] (main.py 268): INFO Test: [370/391]	Time 0.139 (0.143)	Loss 0.6847 (0.7328)	Acc@1 87.500 (83.225)	Acc@5 98.438 (96.231)	Mem 1696MB
[2021-06-18 16:19:36 swin_small_patch4_window7_224] (main.py 268): INFO Test: [380/391]	Time 0.139 (0.143)	Loss 0.7560 (0.7343)	Acc@1 85.156 (83.208)	Acc@5 95.312 (96.231)	Mem 1696MB
[2021-06-18 16:19:38 swin_small_patch4_window7_224] (main.py 268): INFO Test: [390/391]	Time 0.117 (0.143)	Loss 0.8189 (0.7345)	Acc@1 80.000 (83.174)	Acc@5 95.000 (96.230)	Mem 1696MB
[2021-06-18 16:19:38 swin_small_patch4_window7_224] (main.py 274): INFO  * Acc@1 83.174 Acc@5 96.230
[2021-06-18 16:19:38 swin_small_patch4_window7_224] (main.py 123): INFO Accuracy of the network on the 50000 test images: 83.2%
